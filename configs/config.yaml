train_dataset:
  path: /home/wit627/ccshao/lama/celeba-hq-dataset/train_256
  batch_size: 8

val_dataset:
  paths:
    medium: /home/wit627/ccshao/lama/celeba-hq-dataset/val_256/random_medium_256
    thick: /home/wit627/ccshao/lama/celeba-hq-dataset/val_256/random_thick_256
    thin: /home/wit627/ccshao/lama/celeba-hq-dataset/val_256/random_thin_256
  batch_size: 8

visualizer:
  save_path: sample

train_kwargs:
  gpus: 2
  strategy: ddp
  max_epochs: 40
  gradient_clip_val: 1
  limit_train_batches: 25000
#  val_check_interval: ${trainer.kwargs.limit_train_batches}
  # fast_dev_run: True  # uncomment for faster debug
  # track_grad_norm: 2  # uncomment to track L2 gradients norm
  log_every_n_steps: 250
  precision: 16
#  precision: 16
#  amp_backend: native
#  amp_level: O1
  # resume_from_checkpoint: path  # override via command line trainer.resume_from_checkpoint=path_to_checkpoint
  detect_anomaly: False
  # auto_scale_batch_size: True  # uncomment to find largest batch size
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 8
#  limit_val_batches: 1000000
  replace_sampler_ddp: False

checkpoint_kwargs:
  verbose: True
  save_top_k: 5
  save_last: True
  monitor: val_ssim_fid100_f1_total_mean
  mode: max

optimizers:
  generator:
    lr: 0.001
  discriminator:
    lr: 0.001

log:
  tensorboard_logger_path: /home/wit627/ccshao/Inpating/logs
defaults:
  - hydra: overrides
  - generator: FFCResNetGenerator
  - _self_